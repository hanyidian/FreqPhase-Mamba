# mamba + StrongFreq (3-band) å¯è§†åŒ–ç‰ˆè®­ç»ƒè„šæœ¬
import os
import argparse
import random
import torch
import torch.optim as opt
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
import sys
import numpy as np
from tqdm import tqdm
from datetime import datetime
import warnings
import torch.nn as nn
import matplotlib.pyplot as plt  # ç”¨äºå¯è§†åŒ–

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="pytorch_wavelets")

# æ·»åŠ  models ç›®å½•åˆ° Python è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ SAM2UNet
current_dir = os.path.dirname(os.path.abspath(__file__))
models_dir = os.path.join(current_dir, 'models')
sys.path.insert(0, models_dir)

# ä» models ç›®å½•å¯¼å…¥ SAM2UNet æ¨¡å‹åŠ StrongFreqBlockï¼ˆç”¨äºæ³¨å†Œ hookï¼‰
from model9 import SAM2UNet, StrongFreqBlock, InformationFilter
# ä» dataset.py å¯¼å…¥æ•°æ®é›†ç±»
from dataset import FullDataset

# å¯¼å…¥è‡ªå®šä¹‰çš„å·¥å…·å‡½æ•°
from utils import (
    brain_tumor_loss,
    compute_metrics,
    validate,
    plot_training_curve,
    apply_augmentation,
    check_nan_inf,
    val_collate_fn
)

# æ£€æŸ¥æ˜¯å¦æœ‰ amp æ”¯æŒ
try:
    from torch.cuda.amp import GradScaler, autocast
    AMP_AVAILABLE = True
except ImportError:
    AMP_AVAILABLE = False
    print("âš ï¸ æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ ‡å‡†ç²¾åº¦")


# ==================== å¯è§†åŒ–é¢‘åŸŸ & é¢‘å¸¦é—¨æ§çš„è¾…åŠ©å‡½æ•° ====================
# ==================== å¯è§†åŒ–é¢‘åŸŸ & é¢‘å¸¦é—¨æ§çš„è¾…åŠ©å‡½æ•° ====================
# ==================== å¯è§†åŒ–é¢‘åŸŸ & MambaPriorUp çš„è¾…åŠ©å‡½æ•° ====================
def _radial_profile(mag_tensor: torch.Tensor):
    """
    è®¡ç®— 2D é¢‘è°±çš„å¾„å‘å¹³å‡åˆ†å¸ƒï¼Œç”¨äºç”» 1D é¢‘è°±æ›²çº¿ã€‚
    mag_tensor: [H, W] (åœ¨è°ƒç”¨å‰åº” .cpu())
    return: numpy array, shape [R_max+1]
    """
    mag = mag_tensor.numpy()
    H, W = mag.shape

    y, x = np.indices((H, W))
    center_y = (H - 1) / 2.0
    center_x = (W - 1) / 2.0
    r = np.sqrt((x - center_x) ** 2 + (y - center_y) ** 2)
    r = r.astype(np.int64)

    r_max = r.max()
    radial_sum = np.bincount(r.ravel(), weights=mag.ravel(), minlength=r_max + 1)
    radial_cnt = np.bincount(r.ravel(), minlength=r_max + 1)
    radial_mean = radial_sum / (radial_cnt + 1e-8)
    return radial_mean


def visualize_internal(model, val_loader, device, save_root, epoch, max_batches=1):
    """
    é€‚é…å½“å‰ SAM2UNet (model3.py) çš„å†…éƒ¨å¯è§†åŒ–ï¼š

    1) é¢‘åŸŸç¼–ç å™¨ InputFreqEncoder å†…æ¯ä¸ª StrongFreqBlockï¼š
        - é¢‘è°±çƒ­åŠ›å›¾ + å¾„å‘é¢‘ç‡åˆ†å¸ƒ
        - ä¸‰é¢‘å¸¦ gate æŸ±çŠ¶å›¾ (low/mid/high)
        - ä¸‰ä¸ªé¢‘å¸¦å…ˆéªŒ mapï¼ˆlow/mid/highï¼Œé€šé“å¹³å‡ï¼‰

    2) ä¸‰ä¸ªè§£ç å— MambaPriorUp (up1/up2/up3)ï¼š
        - è·¯ç”±æƒé‡ alpha (low/mid/high)
        - ç©ºé—´ gate beta(x)
        - prior_mix çƒ­åŠ›å›¾
        - prior_low / prior_mid / prior_high çƒ­åŠ›å›¾
    """
    import os
    import numpy as np
    import matplotlib.pyplot as plt

    # å…¨å±€é£æ ¼è®¾ç½®
    plt.style.use("seaborn-v0_8")
    plt.rcParams.update({
        "font.size": 12,
        "axes.titlesize": 14,
        "axes.labelsize": 12,
        "figure.figsize": (6, 5),
        "axes.grid": False,
    })

    model.eval()

    # å–ä¸€ä¸ª batch åšåˆ†æ
    try:
        batch = next(iter(val_loader))
    except StopIteration:
        print("âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œæ— æ³•åšå†…éƒ¨å¯è§†åŒ–")
        return

    x = batch["image"].to(device, non_blocking=True)
    x = x[:1]  # åªçœ‹ä¸€å¼ å›¾ï¼Œçœæ˜¾å­˜

    # ç›®æ ‡ç›®å½•
    analysis_dir = os.path.join(save_root, "analysis_v2", f"epoch_{epoch:03d}")
    os.makedirs(analysis_dir, exist_ok=True)

    # ç”¨äºåˆ¤æ–­åˆ°åº•æœ‰æ²¡æœ‰ç”Ÿæˆä»»ä½•å›¾
    made_any_figure = False

    # ------------------------------------------------------------------
    # 1) æ³¨å†Œ StrongFreqBlock çš„ hookï¼šè®°å½• FFT å¹…åº¦ã€radial profileã€ä¸‰å¸¦å…ˆéªŒ map
    # ------------------------------------------------------------------
    debug_state = {
        "freq_mag": [],    # (layer_name, mag[H,W], radial_profile[R])
        "freq_gate": [],   # (layer_name, probs[3])
        "freq_prior": [],  # (layer_name, band_name, prior_map[H,W])
    }
    handles = []

    for name, module in model.named_modules():
        # è¿™é‡Œåªæ•è· freq_encoder é‡Œçš„ StrongFreqBlock
        if isinstance(module, StrongFreqBlock):
            # hook 1: æ•´ä¸ª blockï¼Œæ‹¿è¾“å…¥åš FFTï¼Œæ‹¿è¾“å‡ºçš„ low/mid/high åš map
            def make_freq_block_hook(layer_name):
                def hook(mod, inputs, outputs):
                    with torch.no_grad():
                        x_in = inputs[0].detach().float()  # [B,C,H,W]
                        B, C, H, W = x_in.shape
                        x_ch = x_in[0, 0]  # [H,W]
                        X = torch.fft.fftshift(torch.fft.fft2(x_ch, norm="ortho"))
                        mag = torch.log1p(torch.abs(X))      # [H,W]
                        radial = _radial_profile(mag.cpu())  # [R]

                        debug_state["freq_mag"].append(
                            (layer_name, mag.cpu(), radial)
                        )

                        # outputs: x_fused, x_low, x_mid, x_high
                        if isinstance(outputs, (tuple, list)) and len(outputs) == 4:
                            _, low, mid, high = outputs
                            low_map = low[0].detach().float().mean(dim=0).cpu()   # [H,W]
                            mid_map = mid[0].detach().float().mean(dim=0).cpu()
                            high_map = high[0].detach().float().mean(dim=0).cpu()
                            debug_state["freq_prior"].append(
                                (layer_name, "low", low_map)
                            )
                            debug_state["freq_prior"].append(
                                (layer_name, "mid", mid_map)
                            )
                            debug_state["freq_prior"].append(
                                (layer_name, "high", high_map)
                            )
                return hook

            handles.append(module.register_forward_hook(make_freq_block_hook(name)))

            # hook 2: å¯¹åº”çš„ freq_gateï¼Œæ‹¿ä¸‰é¢‘å¸¦çš„é€šé“æ¦‚ç‡
            if hasattr(module, "freq_gate") and isinstance(module.freq_gate, nn.Module):
                def make_freq_gate_hook(layer_name):
                    def hook(mod, inputs, output):
                        with torch.no_grad():
                            out = output.detach()  # [B,3C,1,1]
                            B2, C3, _, _ = out.shape
                            num_bands = 3
                            if C3 % num_bands != 0:
                                return
                            C = C3 // num_bands
                            out_view = out.view(B2, num_bands, C)  # [B,3,C]
                            probs = torch.softmax(out_view, dim=1)  # [B,3,C]
                            probs_mean = probs.mean(dim=(0, 2))    # [3]
                            debug_state["freq_gate"].append(
                                (layer_name, probs_mean.cpu())
                            )
                    return hook

                handles.append(
                    module.freq_gate.register_forward_hook(make_freq_gate_hook(name))
                )

    # ------------------------------------------------------------------
    # 2) å‰å‘ä¸€æ¬¡ï¼Œè§¦å‘ hookï¼Œæ›´æ–° MambaPriorUp çš„ last_* ç¼“å­˜
    # ------------------------------------------------------------------
    with torch.no_grad():
        _ = model(x)

    # hook ç”¨å®Œè¦ç§»é™¤
    for h in handles:
        h.remove()

    # ------------------------------------------------------------------
    # 3) ç»˜åˆ¶ StrongFreqBlock ç›¸å…³å›¾åƒ
    # ------------------------------------------------------------------
    # 3.1 é¢‘è°± + å¾„å‘æ›²çº¿
    for idx, (layer_name, mag, radial) in enumerate(debug_state["freq_mag"]):
        fig, axes = plt.subplots(1, 2, figsize=(10, 4))
        ax0, ax1 = axes

        im = ax0.imshow(mag.numpy(), cmap="viridis")
        ax0.set_title(f"Freq magnitude\n{layer_name}")
        ax0.set_xticks([])
        ax0.set_yticks([])
        fig.colorbar(im, ax=ax0, fraction=0.046, pad=0.04)

        r = np.linspace(0, 1, len(radial))
        ax1.plot(r, radial / (radial.max() + 1e-8))
        ax1.set_xlabel("Normalized radius (0â†’low, 1â†’high)")
        ax1.set_ylabel("Normalized energy")
        ax1.set_title("Radial frequency profile")

        fig.tight_layout()
        save_path = os.path.join(
            analysis_dir, f"freq_mag_radial_{idx}_{layer_name.replace('.', '_')}.png"
        )
        fig.savefig(save_path, dpi=300)
        plt.close(fig)
        made_any_figure = True

    # 3.2 ä¸‰é¢‘å¸¦ gate æŸ±çŠ¶å›¾
    for idx, (layer_name, probs) in enumerate(debug_state["freq_gate"]):
        probs_np = probs.numpy()  # [3]
        fig, ax = plt.subplots(1, 1, figsize=(4, 4))
        labels = ["low", "mid", "high"]
        ax.bar(labels, probs_np)
        ax.set_ylim(0, 1)
        ax.set_ylabel("Average band probability")
        ax.set_title(f"Freq gates ({layer_name})")
        fig.tight_layout()
        save_path = os.path.join(
            analysis_dir, f"freq_gate_{idx}_{layer_name.replace('.', '_')}.png"
        )
        fig.savefig(save_path, dpi=300)
        plt.close(fig)
        made_any_figure = True

    # 3.3 ä¸‰é¢‘å¸¦å…ˆéªŒ mapï¼ˆä½/ä¸­/é«˜é¢‘ï¼‰
    for idx, (layer_name, band_name, prior_map) in enumerate(debug_state["freq_prior"]):
        fig, ax = plt.subplots(1, 1, figsize=(4, 4))
        im = ax.imshow(prior_map.numpy(), cmap="magma")
        ax.set_title(f"{layer_name} - {band_name} prior (mean over C)")
        ax.set_xticks([])
        ax.set_yticks([])
        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        fig.tight_layout()
        save_path = os.path.join(
            analysis_dir,
            f"freq_prior_{idx}_{layer_name.replace('.', '_')}_{band_name}.png",
        )
        fig.savefig(save_path, dpi=300)
        plt.close(fig)
        made_any_figure = True

    # ------------------------------------------------------------------
    # 4) ç»˜åˆ¶è§£ç å— MambaPriorUp çš„è·¯ç”± & å…ˆéªŒä½¿ç”¨æƒ…å†µ
    # ------------------------------------------------------------------
    for idx, up in enumerate(
        [getattr(model, "up1", None),
         getattr(model, "up2", None),
         getattr(model, "up3", None)],
        start=1
    ):
        if up is None:
            continue

        # 4.1 è·¯ç”±æƒé‡ alpha (low/mid/high)
        if getattr(up, "last_alpha", None) is not None:
            alpha = up.last_alpha[0].detach().cpu().numpy()  # [3]
            fig, ax = plt.subplots(1, 1, figsize=(4, 4))
            labels = ["low", "mid", "high"]
            ax.bar(labels, alpha)
            ax.set_ylim(0, 1)
            ax.set_ylabel("Routing weight Î±")
            ax.set_title(f"Decoder up{idx} routing Î±")
            fig.tight_layout()
            save_path = os.path.join(analysis_dir, f"up{idx}_routing_alpha.png")
            fig.savefig(save_path, dpi=300)
            plt.close(fig)
            made_any_figure = True

        # 4.2 ç©ºé—´ gate Î²(x)
        if getattr(up, "last_beta", None) is not None:
            beta = up.last_beta[0, 0].detach().cpu().numpy()  # [H,W]
            fig, ax = plt.subplots(1, 1, figsize=(4, 4))
            im = ax.imshow(beta, cmap="inferno")
            ax.set_title(f"up{idx} spatial gate Î²(x)")
            ax.set_xticks([])
            ax.set_yticks([])
            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            fig.tight_layout()
            save_path = os.path.join(analysis_dir, f"up{idx}_beta_gate.png")
            fig.savefig(save_path, dpi=300)
            plt.close(fig)
            made_any_figure = True

        # 4.3 prior_mix çƒ­åŠ›å›¾
        if getattr(up, "last_prior_mix", None) is not None:
            prior_mix = up.last_prior_mix[0].detach()  # [C,H,W]
            prior_mix_map = prior_mix.pow(2).mean(dim=0).cpu().numpy()  # [H,W]
            fig, ax = plt.subplots(1, 1, figsize=(4, 4))
            im = ax.imshow(prior_mix_map, cmap="magma")
            ax.set_title(f"up{idx} prior_mix energy (mean over C)")
            ax.set_xticks([])
            ax.set_yticks([])
            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            fig.tight_layout()
            save_path = os.path.join(analysis_dir, f"up{idx}_prior_mix.png")
            fig.savefig(save_path, dpi=300)
            plt.close(fig)
            made_any_figure = True

        if getattr(up, "last_out", None) is not None:
            out_feat = up.last_out[0].detach()  # [C,H,W]
            energy = out_feat.pow(2).mean(dim=0).cpu().numpy()
            fig, ax = plt.subplots(1, 1, figsize=(4, 4))
            im = ax.imshow(energy, cmap="inferno")
            ax.set_title(f"up{idx} output energy (mean over C)")
            ax.set_xticks([])
            ax.set_yticks([])
            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            fig.tight_layout()
            save_path = os.path.join(analysis_dir, f"up{idx}_out_energy.png")
            fig.savefig(save_path, dpi=300)
            plt.close(fig)
            made_any_figure = True

        # 4.4 åˆ†åˆ«ç”» prior_low / prior_mid / prior_highï¼ˆå¯é€‰ï¼‰
        for band_name, tensor in [
            ("low", getattr(up, "last_prior_low", None)),
            ("mid", getattr(up, "last_prior_mid", None)),
            ("high", getattr(up, "last_prior_high", None)),
        ]:
            if tensor is None:
                continue
            pm = tensor[0].detach().float().mean(dim=0).cpu().numpy()  # [H,W]
            fig, ax = plt.subplots(1, 1, figsize=(4, 4))
            im = ax.imshow(pm, cmap="viridis")
            ax.set_title(f"up{idx} {band_name} prior (mean over C)")
            ax.set_xticks([])
            ax.set_yticks([])
            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            fig.tight_layout()
            save_path = os.path.join(
                analysis_dir, f"up{idx}_prior_{band_name}.png"
            )
            fig.savefig(save_path, dpi=300)
            plt.close(fig)
            made_any_figure = True

    if made_any_figure:
        print(f"ğŸ“· æ–°ç‰ˆå†…éƒ¨å¯è§†åŒ–å·²ä¿å­˜åˆ°: {analysis_dir}")
    else:
        print(f"âš ï¸ visualize_internal æ²¡æœ‰ç”Ÿæˆä»»ä½•å›¾ï¼ˆæ£€æŸ¥ StrongFreqBlock hook å’Œ MambaPriorUp.last_* æ˜¯å¦ç”Ÿæ•ˆï¼‰")



# ==================== è§£æå‘½ä»¤è¡Œå‚æ•° ====================
parser = argparse.ArgumentParser("SAM2-UNet Training")
parser.add_argument("--hiera_path", type=str, required=True, help="path to the sam2 pretrained hiera")
parser.add_argument("--train_image_path", type=str, required=True,
                    help="path to the image that used to train the model")
parser.add_argument("--train_mask_path", type=str, required=True, help="path to the mask file for training")
parser.add_argument("--val_ratio", type=float, default=0.1, help="validation set ratio")
parser.add_argument('--save_path', type=str, required=True, help="path to store the checkpoint and results")
parser.add_argument("--epoch", type=int, default=100, help ="training epochs")
parser.add_argument("--lr", type=float, default=0.0005, help="learning rate")
parser.add_argument("--batch_size", default=12, type=int, help="batch size for training and validation")
parser.add_argument("--weight_decay", default=5e-4, type=float)
parser.add_argument("--patience", type=int, default=50, help="early stopping patience")
parser.add_argument("--min_delta", type=float, default=0.0005, help="minimum delta for early stopping")
parser.add_argument("--resume", type=str, default="", help="path to checkpoint for resuming training")
parser.add_argument("--save_val_interval", type=int, default=3, help="save validation results & analysis every N epochs")
parser.add_argument("--max_val_samples", type=int, default=20, help="max validation samples to save per epoch")
args = parser.parse_args()


def main(args):
    # 1. è®¾ç½®éšæœºç§å­
    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # 2. å‡†å¤‡æ•°æ®é›†
    full_dataset = FullDataset(args.train_image_path, args.train_mask_path, 352, mode='train')

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_size = int(len(full_dataset) * args.val_ratio)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=1,
        pin_memory=True,
        collate_fn=val_collate_fn
    )

    # 3. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SAM2UNet(args.hiera_path).to(device)

    start_lr = args.lr
    optimizer = opt.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=start_lr,
        weight_decay=args.weight_decay
    )
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=5,
        min_lr=1e-6
    )

    # 4. åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = None
    if AMP_AVAILABLE and torch.cuda.is_available():
        scaler = GradScaler()
    else:
        print("âš ï¸ ä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")

    # 5. è®­ç»ƒå‡†å¤‡
    os.makedirs(args.save_path, exist_ok=True)
    val_results_dir = os.path.join(args.save_path, "validation_results")
    os.makedirs(val_results_dir, exist_ok=True)

    best_dice = 0.0
    early_stop_counter = 0
    train_losses = []
    val_metrics = {
        'dice': [], 'iou': [], 'precision': [], 'recall': [],
        'f1': [], 'specificity': [], 'accuracy': []
    }

    # æ–­ç‚¹é‡è¿æœºåˆ¶
    start_epoch = 0
    if args.resume:
        if os.path.isfile(args.resume):
            print(f"=> ä»æ£€æŸ¥ç‚¹ '{args.resume}' æ¢å¤è®­ç»ƒ")
            checkpoint = torch.load(args.resume)
            start_epoch = checkpoint.get('epoch', 0)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            best_dice = checkpoint.get('best_dice', 0.0)
            train_losses = checkpoint.get('train_losses', [])
            val_metrics = checkpoint.get('val_metrics', val_metrics)
            if 'scheduler_state_dict' in checkpoint:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            if 'scaler_state_dict' in checkpoint and scaler is not None:
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
            print(f"=> æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹ (epoch {start_epoch})")
        else:
            print(f"=> æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ '{args.resume}'")

    # 6. è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€» epochs: {args.epoch}, åˆå§‹å­¦ä¹ ç‡: {start_lr:.6f}, Batch Size: {args.batch_size}")
    for epoch in range(start_epoch, args.epoch):
        model.train()
        epoch_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{args.epoch}')

        for batch in progress_bar:
            # è½½å…¥æ•°æ®ï¼ˆå…ˆåœ¨ CPU ä¸Šåšå¢å¼ºï¼Œå†æ¬åˆ° GPUï¼‰
            x = batch['image']
            target = batch['label']

            # åº”ç”¨æ•°æ®å¢å¼º
            x, target = apply_augmentation(x, target)

            # ç§»åˆ° GPU
            x = x.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)

            optimizer.zero_grad()

            def forward_and_loss(x, target):
                """å‰å‘ + æŸå¤±è®¡ç®—ï¼ˆåŒæ—¶ç”¨äº AMP å’Œé AMP åˆ†æ”¯ï¼‰"""
                pred0, pred1, pred2 = model(x)

                if check_nan_inf(pred0, "pred0") or \
                   check_nan_inf(pred1, "pred1") or \
                   check_nan_inf(pred2, "pred2"):
                    return None, True  # loss=None, has_nan=True

                # ä¸»è¾“å‡º + è¾…åŠ©è¾“å‡ºï¼ˆæ·±ç›‘ç£ï¼‰
                loss_main = brain_tumor_loss(pred0, target, freq_weight=0.02,boundary_weight=0.05)
                loss_aux2 = brain_tumor_loss(pred1, target, freq_weight=0.02,boundary_weight=0.05)
                loss_aux3 = brain_tumor_loss(pred2, target, freq_weight=0.02,boundary_weight=0.05)

                seg_loss = loss_main + 0.4 * (loss_aux2 + loss_aux3)

                # IFM æ­£åˆ™ï¼ˆå¦‚æœä½ åœ¨ SAM2UNet é‡Œå®ç°äº† get_ifm_reg_lossï¼‰
                ifm_reg = model.get_ifm_reg_loss()
                loss = seg_loss + ifm_reg

                if check_nan_inf(loss, "loss"):
                    return None, True

                return loss, False

            # ========= AMP åˆ†æ”¯ =========
            if scaler is not None:
                with autocast():
                    loss, bad = forward_and_loss(x, target)

                if bad or loss is None:
                    print("è·³è¿‡æ­¤æ‰¹æ¬¡æ›´æ–°å› ä¸ºæ£€æµ‹åˆ° NaN/Inf")
                    continue

                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()

            # ========= æ ‡å‡† FP32 =========
            else:
                loss, bad = forward_and_loss(x, target)

                if bad or loss is None:
                    print("è·³è¿‡æ­¤æ‰¹æ¬¡æ›´æ–°å› ä¸ºæ£€æµ‹åˆ° NaN/Inf")
                    continue

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            # ç»Ÿè®¡ä¸æ—¥å¿—
            epoch_loss += loss.item()
            train_losses.append(loss.item())
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

        # æ˜¯å¦ä¿å­˜å½“å‰ epoch çš„éªŒè¯å¯è§†åŒ–ç»“æœ
        save_results_this_epoch = (epoch + 1) % args.save_val_interval == 0

        # éªŒè¯
        val_metrics_epoch = validate(
            model,
            val_loader,
            device,
            epoch + 1,  # ä½¿ç”¨ 1-based ç´¢å¼•
            val_results_dir,
            save_results=save_results_this_epoch,
            max_val_samples=args.max_val_samples
        )

        # è®°å½•æ‰€æœ‰éªŒè¯æŒ‡æ ‡
        for key, value in val_metrics_epoch.items():
            if key not in val_metrics:
                val_metrics[key] = []
            val_metrics[key].append(value)

        # ä½¿ç”¨ Dice ä½œä¸ºå­¦ä¹ ç‡è°ƒåº¦å™¨çš„æŒ‡æ ‡
        scheduler.step(val_metrics_epoch['dice'])

        # æ‰“å°è¯¦ç»† epoch ç»“æœ
        print(f'\nğŸ“Š Epoch {epoch + 1}/{args.epoch} è¯¦ç»†æŒ‡æ ‡ (LR: {optimizer.param_groups[0]["lr"]:.6f}):')
        print(f'   ğŸ¯ Train Loss: {epoch_loss / max(len(train_loader), 1):.4f}')
        print(f'   ğŸ“ˆ Val Dice: {val_metrics_epoch["dice"]:.4f} (Best: {best_dice:.4f})')
        print(f'   ğŸ¯ Val IoU: {val_metrics_epoch["iou"]:.4f}')
        print(f'   ğŸ” Val Precision: {val_metrics_epoch["precision"]:.4f}')
        print(f'   ğŸ“Š Val Recall: {val_metrics_epoch["recall"]:.4f}')
        print(f'   ğŸª Val F1-Score: {val_metrics_epoch["f1"]:.4f}')
        print(f'   ğŸ›¡ï¸  Val Specificity: {val_metrics_epoch["specificity"]:.4f}')
        print(f'   âœ… Val Accuracy: {val_metrics_epoch["accuracy"]:.4f}')

        if save_results_this_epoch:
            print(f'   ğŸ“¸ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {os.path.join(val_results_dir, f"val_results_epoch_{epoch + 1}")}')
            # æ–°å¢ï¼šåšä¸€æ¬¡å†…éƒ¨é¢‘åŸŸå¯è§†åŒ–
            visualize_internal(
                model,
                val_loader,
                device,
                args.save_path,
                epoch + 1
            )

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        current_dice = val_metrics_epoch['dice']
        if current_dice > best_dice + args.min_delta:
            best_dice = current_dice
            early_stop_counter = 0
            torch.save(model.state_dict(), os.path.join(args.save_path, 'best_model.pth'))
            print(f'ğŸ’¾ New best model saved with Dice: {best_dice:.4f}')
        else:
            early_stop_counter += 1

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0:
            checkpoint_path = os.path.join(args.save_path, f'epoch_{epoch + 1}.pth')
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_dice': best_dice,
                'train_losses': train_losses,
                'val_metrics': val_metrics,
                'scaler_state_dict': scaler.state_dict() if scaler is not None else None
            }
            torch.save(checkpoint, checkpoint_path)
            print(f"ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {checkpoint_path}")

        # æ—©åœæ£€æŸ¥
        if early_stop_counter >= args.patience:
            print(f'\nEarly stopping triggered at epoch {epoch + 1}')
            break

    # 7. è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆç»“æœ
    plot_training_curve(train_losses, val_metrics, args.save_path, args)

    final_checkpoint = {
        'epoch': args.epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'best_dice': best_dice,
        'train_losses': train_losses,
        'val_metrics': val_metrics,
        'scaler_state_dict': scaler.state_dict() if scaler is not None else None
    }
    torch.save(final_checkpoint, os.path.join(args.save_path, 'final_model.pth'))

    print(f'\nTraining completed. Best Val Dice: {best_dice:.4f}')


if __name__ == "__main__":
    start_time = datetime.now().strftime("%Y%m%d-%H%M%S")
    print(f'Training started at {start_time}')

    main(args)

    end_time = datetime.now().strftime("%Y%m%d-%H%M%S")
    print(f'Training completed at {end_time}')
